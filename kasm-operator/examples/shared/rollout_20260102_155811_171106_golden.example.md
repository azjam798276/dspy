---
id: "rollout_20260102_155811_171106_golden"
generated_at: "2026-01-02T16:03:02.809846"
tags: ["shared", "auto-generated"]
---

## Problem
---
id: "20260102_gpu_operator_install"
difficulty: "easy"
tags: ["gpu", "nvidia", "helm", "k3s", "operator"]
tech_stack: "NVIDIA GPU Operator, Helm, K3s"
---

# User Story
As a DevOps engineer, I want to install the NVIDIA GPU Operator on K3s, so the cluster can schedule GPU workloads.

# Context & Constraints
**Prerequisites:**
- NVIDIA driver installed on GPU nodes
- containerd runtime with nvidia-container-runtime
- K3s cluster running

**Installation:**
```bash
# Add NVIDIA Helm repo
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia
helm repo update

# Install GPU Operator
helm install gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator --create-namespace \
  --set driver.enabled=false \
  --set toolkit.enabled=true \
  --set devicePlugin.config.name=time-slicing-confi

## Solution
{
  "session_id": "465921ad-ca48-4f97-ada5-17141a218936",
  "response": "```json\n{\n  \"reasoning\": \"I have encapsulated the installation logic into a shell script (`scripts/install_gpu_operator.sh`) to ensure reproducibility and adherence to the `gpu-engineer` mandates. Crucially, the script explicitly creates the `time-slicing-config` ConfigMap with 8 replicas *before* the Helm installation, ensuring the `--set devicePlugin.config.name=time-slicing-config` argument references a valid configuration. I also utilized `helm upgrade --install` instead of just `install` to make the script idempotent for future runs.\",\n  \"code_changes\": {\n    \"scripts/install_gpu_operator.sh\": \"#!/bin/bash\\nset -euo pipefail\\n\\n# NVIDIA GPU Operator Installation Script\\n# Implements Task: 20260102_gpu_operator_install\\n# Mandates: GPU Time-Slicing (8 Replicas) per gpu-engineer/SKILL.md\\n\\necho \\\"=== 1. Setting up NVIDIA Helm Repository ===\\\"\\nhelm repo add nvidia https://helm.ngc.nvidia.com/nvidia\\nhelm repo update\\n\\necho \\\"=== 2. Configuring GPU Time-Slicing (8 Replicas) ===\\\"\\n# Create namespace first if it doesn't exist\\nkubectl create namespace gpu-operator --dry-run=client -o yaml | kubectl apply -f -\\n\\n# Apply the ConfigMap mandated by gpu-engineer skill\\ncat <<EOF | kubectl apply -f -\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: time-slicing-config\\n  namespace: gpu-operator\\ndata:\\n  any: |-\\n    version: v1\\n    flags:\\n      migStrategy: none\\n    sharing:\\n      timeSlicing:\\n        renameByDefault: false\\n        failRequestsGreaterThanOne: false\\n        resources:\\n        - name: nvidia.com/gpu\\n          replicas: 8\\nEOF\\n\\necho \\\"=== 3. Installing GPU Operator ===\\\"\\n# Driver is disabled (assuming pre-installed on node)\\n# devicePlugin.config.name must match the ConfigMap created above\\nhelm upgrade --install gpu-operator nvidia/gpu-operator \\\\\\n  --namespace gpu-operator \\\\\\n  --create-namespace \\\\\\n  --set driver.enabled=false \\\\\\n  --set toolkit.enabled=true \\\\\\n  --set devicePlugin.config.name=time-slicing-config\\n\\necho \\\"=== Installation Complete ===\\\"\\necho \\\"Verify with: kubectl get pods -n gpu-operator\\\"\\necho \\\"Verify capacity: kubectl get nodes -o custom-columns=NAME:.metadata.name,GPU:.status.capacity.nvidia\\\\\\\\.com/gpu\\\"\\n\"\n  }\n}\n```",
  "stats": {
    "models": {
      "gemini-2.5-flash-lite": {
        "api": {
          "totalRequests": 1,
          "totalErrors": 0,
          "totalLatencyMs": 2011
        },
        "tokens": {
          "input": 5836,
          "prompt": 5836,
          "candidates": 80,
          "total": 6282,
          "cached": 0,
          "thoughts": 366,
          "tool": 0
        }
      },
      "gemini-3-pro-preview": {
        "api": {
          "totalRequests": 3,
          "totalErrors": 0,
          "totalLatencyMs": 61968
        },
        "tokens": {
          "input": 65285,
          "

## Key Techniques
- JWT token handling
