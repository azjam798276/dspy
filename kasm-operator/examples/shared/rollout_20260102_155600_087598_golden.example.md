---
id: "rollout_20260102_155600_087598_golden"
generated_at: "2026-01-02T16:03:02.789727"
tags: ["shared", "auto-generated"]
---

## Problem
---
id: "20260102_gpu_time_slicing"
difficulty: "medium"
tags: ["gpu", "nvidia", "time-slicing", "device-plugin", "k8s"]
tech_stack: "NVIDIA GPU Operator, K3s, ConfigMap"
---

# User Story
As a GPU engineer, I want to configure NVIDIA time-slicing on K3s, so multiple VDI sessions can share a single GPU efficiently.

# Context & Constraints
**Time-Slicing Configuration:**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: time-slicing-config
  namespace: gpu-operator
data:
  any: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        renameByDefault: false
        failRequestsGreaterThanOne: false
        resources:
        - name: nvidia.com/gpu
          replicas: 8
```

**ClusterPolicy Patch:**
```bash
kubectl patch clusterpolicy/cluster-polic

## Solution
{
  "session_id": "0d18e13e-06c9-4336-8eb4-5165b303e2f8",
  "response": "Reasoning:\nTo enable NVIDIA GPU time-slicing for the Kasm Operator, we need to define a sharing configuration that splits a single physical GPU into multiple schedulable units (replicas). Based on the requirements, we target 8 replicas per GPU.\n\nThe implementation involves two key artifacts:\n1.  **`manifests/gpu-time-slicing.yaml`**: A Kubernetes `ConfigMap` that defines the time-slicing policy (8 replicas for `nvidia.com/gpu`). This file adheres to the schema expected by the NVIDIA GPU Operator.\n2.  **`scripts/configure_gpu_slicing.sh`**: A shell script to automate the deployment. It first applies the ConfigMap and then patches the `ClusterPolicy` of the GPU Operator to reference this new configuration. This ensures the changes are applied consistently and documents the necessary imperative commands.\n\nThis approach satisfies the acceptance criteria by creating the config, providing the patch mechanism, and setting up the system for the required density.\n\n```json\n[\n  {\n    \"action\": \"write\",\n    \"path\": \"manifests/gpu-time-slicing.yaml\",\n    \"content\": \"apiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: time-slicing-config\\n  namespace: gpu-operator\\ndata:\\n  any: |-\\n    version: v1\\n    flags:\\n      migStrategy: none\\n    sharing:\\n      timeSlicing:\\n        renameByDefault: false\\n        failRequestsGreaterThanOne: false\\n        resources:\\n        - name: nvidia.com/gpu\\n          replicas: 8\"\n  },\n  {\n    \"action\": \"write\",\n    \"path\": \"scripts/configure_gpu_slicing.sh\",\n    \"content\": \"#!/bin/bash\\nset -e\\n\\n# 1. Apply the Time-Slicing ConfigMap\\necho \\\"Applying time-slicing configuration...\\\"\\nkubectl apply -f manifests/gpu-time-slicing.yaml\\n\\n# 2. Patch the GPU Operator ClusterPolicy\\n# This instructs the Operator to use the config we just applied\\necho \\\"Patching GPU Operator ClusterPolicy...\\\"\\nkubectl patch clusterpolicy/cluster-policy \\\\\\n  -n gpu-operator --type merge \\\\\\n  -p '{\\\"spec\\\":{\\\"devicePlugin\\\":{\\\"config\\\":{\\\"name\\\":\\\"time-slicing-config\\\"}}}}'\\n\\n# 3. Verification Instructions\\necho \\\"Configuration applied successfully.\\\"\\necho \\\"To verify, run: kubectl describe node <node-name> | grep nvidia.com/gpu\\\"\\necho \\\"Expected output: nvidia.com/gpu: 8\\\"\"\n  }\n]\n```",
  "stats": {
    "models": {
      "gemini-2.5-flash-lite": {
        "api": {
          "totalRequests": 1,
          "totalErrors": 0,
          "totalLatencyMs": 2513
        },
        "tokens": {
          "input": 5781,
          "prompt": 5781,
          "candidates": 83,
          "total": 6234,
          "cached": 0,
          "thoughts": 370,
          "tool": 0
        }
      },
      "gemini-3-pro-preview": {
        "api": {
          "totalRequests": 5,
          "totalErrors": 0,
          "totalLatencyMs": 52309
        },
        "tokens": {
         

## Key Techniques
- JWT token handling
- Input validation
