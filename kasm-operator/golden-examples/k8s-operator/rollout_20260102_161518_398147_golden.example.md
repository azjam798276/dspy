---
id: "rollout_20260102_161518_398147_golden"
generated_at: "2026-01-02T16:24:01.936938"
tags: ["k8s-operator", "auto-generated"]
---

## Problem
---
id: "20260101_init_container_debug"
difficulty: "hard"
tags: ["troubleshooting", "kubernetes", "init-containers", "kasm", "debugging"]
tech_stack: "Kubernetes, K3s, Kasm Workspaces, PostgreSQL, Redis"
---

# User Story
As a K8s operator, I want to diagnose and fix Kasm pods stuck in Init:X/Y state by identifying failing init containers and their root causes.

# Context & Constraints
Debug kasm-proxy-default-deployment stuck in Init:1/5:

**Init Container Sequence:**
1. wait-for-db: Waits for PostgreSQL
2. wait-for-redis: Waits for Redis
3. init-config: Generates configuration
4. init-certs: Sets up TLS certificates
5. db-migrate: Runs database migrations

**Symptoms:**
- Pod status shows Init:1/5 (second container failing)
- Pod never reaches Running state
- Services unavailable

**Pot

## Solution
{
  "session_id": "07ebee88-e156-4680-9f23-3d693da7dbad",
  "response": "```json\n{\n  \"reasoning\": \"Based on the symptom `Init:1/5`, the first init container (`wait-for-db`) has successfully completed, and the pod is stuck on the second container (`wait-for-redis`).\\n\\n**Diagnosis Strategy:**\\n1.  **Isolate the Failure**: The `wait-for-redis` container typically checks for TCP connectivity to the Redis service. If it hangs or fails, it implies a network, configuration, or service availability issue.\\n2.  **Automated Troubleshooting**: I will create a script `scripts/diagnose_kasm_init.sh` to automatically identify the stuck pod, inspect the status of the init containers, and extract logs from `wait-for-redis`. It will also verify the existence and endpoints of the `kasm-redis` service.\\n3.  **Likely Root Cause & Fix**: A common 'hard' configuration issue in locked-down environments is a `NetworkPolicy` blocking internal traffic. I will provide a remediation manifest `manifests/fixes/allow-redis-traffic.yaml` that explicitly allows traffic from `kasm-proxy` to `kasm-redis` on port 6379, addressing the 'Network policy blocking traffic' potential cause.\",\n  \"changes\": [\n    {\n      \"path\": \"kasm-operator/scripts/diagnose_kasm_init.sh\",\n      \"content\": \"#!/bin/bash\\nset -e\\n\\n# Usage: ./diagnose_kasm_init.sh [namespace]\\nNAMESPACE=${1:-default}\\nDEPLOYMENT=\\\"kasm-proxy-default-deployment\\\"\\nLABEL_SELECTOR=\\\"app=kasm-proxy\\\"\\n\\necho \\\"Starting diagnosis for Kasm Proxy in namespace: $NAMESPACE\\\"\\n\\n# 1. Find the Pod\\nPOD=$(kubectl get pods -n \\\"$NAMESPACE\\\" -l \\\"$LABEL_SELECTOR\\\" --field-selector=status.phase=Pending -o jsonpath=\\\"{.items[0].metadata.name}\\\")\\n\\nif [ -z \\\"$POD\\\" ]; then\\n    echo \\\"[WARN] No Pending pods found for $LABEL_SELECTOR. Checking all pods...\\\"\\n    POD=$(kubectl get pods -n \\\"$NAMESPACE\\\" -l \\\"$LABEL_SELECTOR\\\" -o jsonpath=\\\"{.items[0].metadata.name}\\\")\\nfi\\n\\nif [ -z \\\"$POD\\\" ]; then\\n    echo \\\"[ERROR] No pods found for deployment $DEPLOYMENT\\\"\\n    exit 1\\nfi\\n\\necho \\\"[INFO] Investigating Pod: $POD\\\"\\n\\n# 2. Check Init Container Status\\nINIT_STATUS=$(kubectl get pod \\\"$POD\\\" -n \\\"$NAMESPACE\\\" -o jsonpath=\\\"{.status.initContainerStatuses}\\\")\\necho \\\"[INFO] Init Container Statuses retrieved.\\\"\\n\\n# Check specifically for the 2nd container (index 1) which matches Init:1/5\\nWAIT_REDIS_STATE=$(echo \\\"$INIT_STATUS\\\" | jq -r '.[1].state')\\nWAIT_REDIS_READY=$(echo \\\"$INIT_STATUS\\\" | jq -r '.[1].ready')\\n\\necho \\\"[INFO] wait-for-redis State: $WAIT_REDIS_STATE\\\"\\n\\n# 3. Extract Logs from wait-for-redis\\necho \\\"--- LOGS: wait-for-redis ---\\\"\\nkubectl logs \\\"$POD\\\" -c wait-for-redis -n \\\"$NAMESPACE\\\" --tail=50\\necho \\\"----------------------------\\\"\\n\\n# 4. Check Redis Service Health\\necho \\\"[INFO] Checking Redis Service...\\\"\\nkubectl get svc kasm-redis -n \\\"$NAMESP

## Key Techniques
- JWT token handling
