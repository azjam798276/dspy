---
id: "rollout_20251226_102618_730028_golden"
generated_at: "2026-01-02T16:24:01.932275"
tags: ["k8s-operator", "auto-generated"]
---

## Problem
---
id: "20251226_logging_elk"
difficulty: "medium"
tags: ["devops", "logging", "elk", "observability"]
tech_stack: "Node.js, Winston, Filebeat, Elasticsearch, Kibana"
---

# User Story
As a DevOps engineer, I want to implement structured logging with centralized log aggregation.

# Context & Constraints
Set up ELK stack integration:

**Logging Requirements:**
- Structured JSON logs with Winston
- Request ID correlation across services
- Log levels (debug, info, warn, error)
- Sensitive data redaction

**Stack:**
- Winston for Node.js logging
- Filebeat for log shipping
- Elasticsearch for storage
- Kibana for visualization

**Technical Constraints:**
- Logs must be JSON formatted
- Include trace ID from headers
- Redact PII (emails, tokens, passwords)
- Index pattern per environment

# Ac

## Solution
{
  "session_id": "08fb32ca-4e94-4c50-9cce-5c419f89622a",
  "response": "```json\n{\n  \"reasoning\": \"## Implementation Plan\\n\\n1.  **Structured Logging (Winston):**\\n    *   **Complexity:** O(1) per log operation.\\n    *   **Configuration:** Use `winston.format.json()` for structured output compatible with ELK. Add `timestamp`. output to Console and File.\\n    *   **PII Redaction:** Implement a custom Winston format (`piiRedactor`) to recursively scrub sensitive keys (email, password, token) from log objects before serialization. This ensures `Security & Reliability` compliance.\\n\\n2.  **Request Correlation:**\\n    *   **Middleware:** Create `requestIdMiddleware` to generate a UUID v4 if `x-request-id` header is missing.\\n    *   **Propagation:** Attach `id` to `req` and set `x-request-id` header on `res` for downstream services.\\n\\n3.  **HTTP Logging:**\\n    *   **Middleware:** Implement `httpLogger` to log request details (method, URL, status, duration) *after* the response finishes (`res.on('finish')`), ensuring accurate duration and status capture.\\n\\n4.  **Log Shipping (Filebeat):**\\n    *   **Config:** `filebeat.yml` configured to read the JSON log file, parse JSON fields to root (for easier querying in Kibana), and output to Elasticsearch.\\n\\n5.  **Kibana:**\\n    *   **Index Pattern:** Define `logs-backend-*` pattern matching the Filebeat output index.\",\n  \"code_patch\": \"diff --git a/filebeat.yml b/filebeat.yml\\nnew file mode 100644\\nindex 0000000..e69de29\\n--- /dev/null\\n+++ b/filebeat.yml\\n@@ -0,0 +1,21 @@\\n+filebeat.inputs:\\n+- type: log\\n+  enabled: true\\n+  paths:\\n+    - /app/app.log\\n+  json.keys_under_root: true\\n+  json.overwrite_keys: true\\n+  json.add_error_key: true\\n+\\n+processors:\\n+  - add_host_metadata: ~\\n+  - add_cloud_metadata: ~\\n+\\n+output.elasticsearch:\\n+  hosts: [\\\"${ELASTICSEARCH_HOST:elasticsearch:9200}\\\"]\\n+  username: \\\"${ELASTICSEARCH_USERNAME}\\\"\\n+  password: \\\"${ELASTICSEARCH_PASSWORD}\\\"\\n+  index: \\\"logs-backend-%{+yyyy.MM.dd}\\\"\\n+\\n+setup.kibana:\\n+  host: \\\"${KIBANA_HOST:kibana:5601}\\\"\\n+\\n+setup.template.name: \\\"logs-backend\\\"\\n+setup.template.pattern: \\\"logs-backend-*\\\"\\ndiff --git a/kibana/index-pattern.json b/kibana/index-pattern.json\\nnew file mode 100644\\nindex 0000000..e69de29\\n--- /dev/null\\n+++ b/kibana/index-pattern.json\\n@@ -0,0 +1,6 @@\\n+{\\n+  \\\"attributes\\\": {\\n+    \\\"title\\\": \\\"logs-backend-*\\\",\\n+    \\\"timeFieldName\\\": \\\"@timestamp\\\"\\n+  }\\n+}\\ndiff --git a/src/middleware/httpLogger.js b/src/middleware/httpLogger.js\\nnew file mode 100644\\nindex 0000000..e69de29\\n--- /dev/null\\n+++ b/src/middleware/httpLogger.js\\n@@ -0,0 +1,25 @@\\n+const logger = require('../utils/logger');\\n+\\n+/**\\n+ * Middleware to log HTTP requests\\n+ * @param {import('express').Request} req \\n+ * @param {import('express').Response} res \\n+ * @param {import('express').NextFunction} next \\n+ */\\

## Key Techniques
- JWT token handling
- Middleware pattern
