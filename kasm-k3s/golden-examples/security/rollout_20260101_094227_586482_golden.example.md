---
id: "rollout_20260101_094227_586482_golden"
generated_at: "2026-01-02T02:38:39.822737"
tags: ["security-engineer", "auto-generated"]
---

## Problem
---
id: "20260101_init_container_debug"
difficulty: "hard"
tags: ["troubleshooting", "kubernetes", "init-containers", "kasm", "debugging"]
tech_stack: "Kubernetes, K3s, Kasm Workspaces, PostgreSQL, Redis"
---

# User Story
As a K8s operator, I want to diagnose and fix Kasm pods stuck in Init:X/Y state by identifying failing init containers and their root causes.

# Context & Constraints
Debug kasm-proxy-default-deployment stuck in Init:1/5:

**Init Container Sequence:**
1. wait-for-db: Waits for PostgreSQL
2. wait-for-redis: Waits for Redis
3. init-config: Generates configuration
4. init-certs: Sets up TLS certificates
5. db-migrate: Runs database migrations

**Symptoms:**
- Pod status shows Init:1/5 (second container failing)
- Pod never reaches Running state
- Services unavailable

**Pot

## Solution
{
  "session_id": "51bfd6b9-93fd-4038-b81e-4aec10c303ed",
  "response": "```json\n{\n  \"reasoning\": \"The pod status 'Init:1/5' combined with the symptom 'second container failing' indicates that the first init container (wait-for-db) completed successfully, and the deployment is blocked at the second container (wait-for-redis). This suggests the kasm-redis service is unhealthy or unreachable. In K3s environments, a common root cause for backing services failing to start is the `local-path-provisioner` defaulting to `0700` (root-only) permissions, which prevents non-root containers (like Redis, often UID 1001) from writing to their persistent volumes. The remediation involves diagnosing the specific error logs, patching the storage provisioner to allow `0777` permissions, and recycling the affected Redis PVC/Pod to ensure the volume is re-provisioned with the correct access rights.\",\n  \"code_changes\": {\n    \"filename\": \"fix_kasm_redis_init.sh\",\n    \"content\": \"#!/bin/bash\\nset -euo pipefail\\n\\n# 1. Diagnostic: Check logs for the failing init container (wait-for-redis)\\necho \\\">>> Checking logs for failing init container (wait-for-redis)...\\\"\\nkubectl logs -n kasm -l app=kasm-proxy -c wait-for-redis --tail=20 || true\\n\\n# 2. Diagnostic: Inspect Redis pod state and events\\n# Look for 'CrashLoopBackOff' or 'Pending' and 'FailedMount' events\\necho \\\">>> Checking kasm-redis pod status and events...\\\"\\nkubectl get pods -n kasm -l app=kasm-redis\\nkubectl describe pod -n kasm -l app=kasm-redis | grep -A 10 \\\"Events\\\"\\n\\n# 3. Fix: Patch K3s local-path-provisioner for non-root access\\n# Changes permission mode from default 0700 to 0777 to allow non-root UIDs to write\\necho \\\">>> Patching local-path-config to use mode 0777...\\\"\\nkubectl patch cm local-path-config -n kube-system --type=merge -p '{\\\"data\\\":{\\\"setupCommand\\\":\\\"/opt/local-path-provisioner/bin/entrypoint.sh -D /opt/local-path-provisioner/bin/ -p -m 0777\\\"}}'\\n\\n# 4. Apply: Restart the provisioner to load new config\\necho \\\">>> Restarting local-path-provisioner...\\\"\\nkubectl delete pod -l app=local-path-provisioner -n kube-system --wait=true\\n\\n# 5. Recovery: Recycle Redis PVC and Pod\\n# This forces the volume to be re-created with the new 0777 permissions\\n# WARNING: This destroys existing data in the Redis volume\\necho \\\">>> Recycling Redis PVC and Pod...\\\"\\nkubectl delete pvc -n kasm -l app=kasm-redis --wait=true\\nkubectl delete pod -n kasm -l app=kasm-redis --wait=true\\n\\n# 6. Verification\\necho \\\">>> Waiting for kasm-proxy deployment to proceed...\\\"\\nkubectl rollout status deployment/kasm-proxy -n kasm --timeout=120s\"\n  }\n}\n```",
  "stats": {
    "models": {
      "gemini-2.5-flash-lite": {
        "api": {
          "totalRequests": 1,
          "totalErrors": 0,
          "totalLatencyMs": 2138
        },
        "tokens": {
          "input": 5855,
          "prompt": 5855,
          "candidates": 11

## Key Techniques
- JWT token handling
