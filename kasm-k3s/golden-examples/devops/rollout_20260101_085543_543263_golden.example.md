---
id: "rollout_20260101_085543_543263_golden"
generated_at: "2026-01-02T02:40:10.746961"
tags: ["devops-engineer", "auto-generated"]
---

## Problem
---
id: "20260101_init_container_debug"
difficulty: "hard"
tags: ["troubleshooting", "kubernetes", "init-containers", "kasm", "debugging"]
tech_stack: "Kubernetes, K3s, Kasm Workspaces, PostgreSQL, Redis"
---

# User Story
As a K8s operator, I want to diagnose and fix Kasm pods stuck in Init:X/Y state by identifying failing init containers and their root causes.

# Context & Constraints
Debug kasm-proxy-default-deployment stuck in Init:1/5:

**Init Container Sequence:**
1. wait-for-db: Waits for PostgreSQL
2. wait-for-redis: Waits for Redis
3. init-config: Generates configuration
4. init-certs: Sets up TLS certificates
5. db-migrate: Runs database migrations

**Symptoms:**
- Pod status shows Init:1/5 (second container failing)
- Pod never reaches Running state
- Services unavailable

**Pot

## Solution
{
  "session_id": "85cabb41-f0a6-42cb-b541-fcef46855beb",
  "response": "```json\n{\n  \"reasoning\": \"The 'Init:1/5' status indicates the 'wait-for-db' container is failing to connect to the PostgreSQL database. This blockage prevents the Kasm proxy from starting. Based on the K3s/Kasm deployment context, the two primary suspects are: 1) Storage Permissions: The K3s 'local-path' provisioner defaults to 0700 permissions, preventing the non-root PostgreSQL container (UID 1001) from writing to its volume, leading to a CrashLoopBackOff. 2) Network Isolation: A 'Default Deny' NetworkPolicy might be blocking the init container's connection attempt to 'kasm-db:5432'. The solution involves a diagnostic script to pinpoint the failure (Pod status vs. Network) and a fix script that patches the storage provisioner and ensures internal connectivity.\",\n  \"code_changes\": {\n    \"diagnose_init_1_5.sh\": \"#!/bin/bash\\nset -euo pipefail\\nNAMESPACE=\\\"kasm\\\"\\n\\necho \\\"=== Kasm Init:1/5 Diagnostic ===\\\"\\n\\n# 1. Identify Blocking Init Container\\necho \\\"[1] Checking Pod Init Status...\\\"\\nPOD=$(kubectl get pods -n $NAMESPACE -l app=kasm-proxy -o name | head -1)\\nkubectl get $POD -n $NAMESPACE -o jsonpath='{.status.initContainerStatuses[*].name}' | tr ' ' '\\\\n' | nl\\n\\n# 2. Check Database Pod Health (Target of Init:1/5)\\necho \\\"\\\\n[2] Checking Database Pod (kasm-db)...\\\"\\nDB_STATUS=$(kubectl get pods -n $NAMESPACE -l app=kasm-db --no-headers | awk '{print $3}')\\necho \\\"DB Pod Status: $DB_STATUS\\\"\\n\\nif [[ \\\"$DB_STATUS\\\" != \\\"Running\\\" ]]; then\\n  echo \\\"-> FAILURE: Database is not running.\\\"\\n  echo \\\"-> Checking for PVC issues (Pending?)...\\\"\\n  kubectl get pvc -n $NAMESPACE -l app=kasm-db\\n  echo \\\"-> Checking for CrashLoopBackOff (Logs)...\\\"\\n  kubectl logs -n $NAMESPACE -l app=kasm-db --tail=20\\nelse\\n  echo \\\"-> SUCCESS: Database is running.\\\"\\n  # 3. Check Connectivity (Only if DB is running)\\n  echo \\\"\\\\n[3] Testing Connectivity...\\\"\\n  kubectl run -it --rm debug-net --image=busybox -n $NAMESPACE --restart=Never -- nc -zv kasm-db 5432\\nfi\\n\",\n    \"fix_init_1_5.sh\": \"#!/bin/bash\\nset -euo pipefail\\nNAMESPACE=\\\"kasm\\\"\\n\\necho \\\"=== Applying Fixes for Init:1/5 ===\\\"\\n\\n# Fix 1: K3s local-path Permission Denied (Root Cause for DB CrashLoop)\\n# Updates provisioner to create volumes with 0777 so non-root DB can write.\\necho \\\"[1] Patching local-path-provisioner permissions...\\\"\\nkubectl patch cm local-path-config -n kube-system --type=merge -p '{\\\"data\\\":{\\\"setupCommand\\\":\\\"/opt/local-path-provisioner/bin/entrypoint.sh -D /opt/local-path-provisioner/bin/ -p -m 0777\\\"}}'\\n\\n# Restart provisioner to apply config\\nkubectl delete pod -l app=local-path-provisioner -n kube-system\\necho \\\"-> local-path-provisioner patched.\\\"\\n\\n# Fix 2: Ensure Network Connectivity\\n# Applies a NetworkPolicy allowing internal traffic within the namespace.\\n

## Key Techniques
- JWT token handling
