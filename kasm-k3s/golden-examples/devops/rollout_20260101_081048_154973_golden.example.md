---
id: "rollout_20260101_081048_154973_golden"
generated_at: "2026-01-02T02:40:10.748411"
tags: ["devops-engineer", "auto-generated"]
---

## Problem
---
id: "20260101_init_container_debug"
difficulty: "hard"
tags: ["troubleshooting", "kubernetes", "init-containers", "kasm", "debugging"]
tech_stack: "Kubernetes, K3s, Kasm Workspaces, PostgreSQL, Redis"
---

# User Story
As a K8s operator, I want to diagnose and fix Kasm pods stuck in Init:X/Y state by identifying failing init containers and their root causes.

# Context & Constraints
Debug kasm-proxy-default-deployment stuck in Init:1/5:

**Init Container Sequence:**
1. wait-for-db: Waits for PostgreSQL
2. wait-for-redis: Waits for Redis
3. init-config: Generates configuration
4. init-certs: Sets up TLS certificates
5. db-migrate: Runs database migrations

**Symptoms:**
- Pod status shows Init:1/5 (second container failing)
- Pod never reaches Running state
- Services unavailable

**Pot

## Solution
{
  "session_id": "7e2e0e3b-992c-44e4-bef6-f3f71118e783",
  "response": "Based on the symptoms described in the story and the diagnosis patterns in the TDD and Golden Examples, I have identified the root cause as a likely permissions issue with the K3s `local-path` storage provisioner preventing the Redis container (a dependency for the failing init container) from starting. I have generated a fix script that adjusts the provisioner configuration and recreates the affected volume.\n\n```json\n{\n  \"reasoning\": \"The pod `kasm-proxy-default-deployment` is stuck in `Init:1/5`, which corresponds to the 2nd init container `wait-for-redis` (index 1) failing to complete. This implies the `kasm-redis` service is unreachable.\\n\\nRoot Cause Analysis:\\n1.  **Dependency Failure**: The `wait-for-redis` container is looping because the `kasm-redis` pod is likely not Ready.\\n2.  **K3s Storage Constraint**: K3s uses the `local-path` provisioner by default, which creates host directories with `0700` (root-only) permissions. If the Redis container image runs as a non-root user (standard security practice), it will fail to write to its mounted PVC, resulting in a `CrashLoopBackOff` or `Pending` state due to `Permission denied` errors.\\n3.  **Confirmation**: This matches the 'Golden Example' scenario for DB failures, applicable here to Redis as well.\\n\\nCorrective Action:\\n1.  Patch the `local-path-config` ConfigMap in the `kube-system` namespace to use `mkdir -m 0777` instead of `0700`, allowing non-root access.\\n2.  Restart the provisioner to apply the config.\\n3.  Delete the Redis PVC and Pod to force the creation of a new volume with the correct permissions.\",\n  \"code_changes\": {\n    \"solutions/fix_redis_init_stall.sh\": \"#!/bin/bash\\nset -euo pipefail\\n\\nNAMESPACE=\\\"kasm\\\"\\nREDIS_APP=\\\"kasm-redis\\\"\\n\\necho \\\"=== Kasm Init:1/5 Debug & Fix ===\\\"\\n\\n# 1. Identify the blocking Pod\\necho \\\"[1/5] Identifying stuck proxy pod...\\\"\\nPOD=$(kubectl get pods -n $NAMESPACE -l app=kasm-proxy --field-selector=status.phase=Pending -o name | head -1)\\nif [ -z \\\"$POD\\\" ]; then\\n  echo \\\"No pending proxy pods found. Exiting.\\\"\\n  exit 0\\nfi\\n\\n# 2. Verify Init Container State (Index 1 = wait-for-redis)\\necho \\\"[2/5] Checking init container status for $POD...\\\"\\nINIT_NAME=$(kubectl get $POD -n $NAMESPACE -o jsonpath='{.status.initContainerStatuses[1].name}')\\nINIT_READY=$(kubectl get $POD -n $NAMESPACE -o jsonpath='{.status.initContainerStatuses[1].ready}')\\n\\nif [ \\\"$INIT_NAME\\\" == \\\"wait-for-redis\\\" ] && [ \\\"$INIT_READY\\\" == \\\"false\\\" ]; then\\n  echo \\\"Verified: Pod is stuck on '$INIT_NAME'.\\\"\\nelse\\n  echo \\\"Mismatch: Pod is stuck on '$INIT_NAME' (Ready: $INIT_READY). Expected 'wait-for-redis'. Aborting fix.\\\"\\n  exit 1\\nfi\\n\\n# 3. Apply Local-Path Permission Fix\\n# Changes default directory creation from 0700 to 0777 for non-root containers\\necho \\\"[3/5] Patching local-path-

## Key Techniques
- JWT token handling
